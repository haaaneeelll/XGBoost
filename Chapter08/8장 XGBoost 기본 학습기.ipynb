{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a2a2d5e",
   "metadata": {},
   "source": [
    "# XGBoost 기본 학습기\n",
    "- 이 장에서는 XGBoost에 있는 다양한 기본 학습기를 분석하고 적용한다.\n",
    "- XGBoost에서 기본 학습기는 부스팅 단계마다 반복적으로 사용되는 개별 모델이다.\n",
    "- 가장 많이 사용하는 것은 트리이다. XGBoost에서 gbtree로 설정할 수 있는 기본 결정 트리 외에도 기본 학습기를 위한 추가 옵션에는 gblinear와 dart가 있다. 또한 XGBoost는 기본 학습기와 트리 앙상블 알고리즘으로 랜덤 포레스트를 제공한다.\n",
    "- 이장에서 실험해보겠다.\n",
    "---\n",
    "- 다른 기본 학습기를 적요하는 방법을 배우면 XGBoost에 대한 지식을 크게 확장할 수 있다. \n",
    "- 더 많은 모델을 구축할 수 있는 능력을 갖게 될 것이며, 선형, 트리 기반, 랜덤 포레스트 머신러닝 알고리즘을 개발하는 새로운 접근 방식을 배우게 될 것이다.\n",
    "- 이 장의 목표는 고급 XGBoost 옵션을 활용하여 다양한 상황에 가장 적합한 모델을 찾을 수 있도록 여러 가지 기본학습기로 XGBoot 모델을 구축하는데 능숙해지는 것이다.\n",
    "> 이장에서는 다음과 같은 내용을 다룬다.\n",
    "- 여러가지 기본 학습기\n",
    "- gblinear 적용하기\n",
    "- dart 비교하기\n",
    "- xgboost 랜덤 포레스트 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b299e2",
   "metadata": {},
   "source": [
    "# 8.1 여러 가지 기본 학습기\n",
    "- 기본 학습기는 XGBoost가 앙상블 모델을 만드는 데 사용하는 머신러닝 모델이다.\n",
    "- 모델이기 때문에 base라는 단어를 사용하고 모델이 오류로부터 학습하기 때문에 leaner라는 단어를 사용한다.\n",
    "---\n",
    "- 결정 트리는 부스팅했을 때 우수한 성능을 제공하기 때문에 XGBoost의 기본 학습기로 선호된다.\n",
    "- 결정 트리의 인기는 XGBoost를 넘어 랜덤 포레스트와 극도로 랜덤화 된 트리(사이킷런의 ExtraTreesClassifier와 ExtraTreesRegressor 문서를 참고하세요)와 같은 다른 앙상블 알고리즘으로 확장되었다.\n",
    "---\n",
    "- XGBoost에서 gbtree로 알려진 기본값은 여러 기본 학습기 중 하나이다.\n",
    "- 그레이디언트 부스팅 선형 모델인 gblinear, 신경망의 드롭아웃 기법을 적용한 결정트리의 변형인 dart가 있다.\n",
    "- 게다가 XGBoost 랜덤포레스트도 있다. 다음 절에서 이런 기본 학습기의 차이점을 알아보고 이어지는 절에서 여러 가지 학습기를 적용해보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24e4c62",
   "metadata": {},
   "source": [
    "## 8.1.1 gblinear\n",
    "- 결정 트리는 비선형 데이터에 최적이다. 데이터를 필요한 만큼 분할하여 샘플에 쉽게 도달할 수 있다.\n",
    "- 실제 데이터는 일반적으로 비선형이기 때문에 기본학습기로 결정트리가 종종 선호된다.\n",
    "- 하지만 선형 모델이 적합한 경우도 있다. 실제 데이터가 선형 관계를 가지고 있다면 결정 트리가 최선의 선택이 아닐 것이다.\n",
    "- 이런 경우를 위해 XGBoost는 선형 기본 학습기인 gblinear를 제공한다.\n",
    "---\n",
    "- 선형 부스팅 모델의 일반적인 아이디어는 트리 부스팅 모델과 동일하다. 기본 모델을 만들고 이어지는 후속 모델이 잔차를 바탕으로 훈련된다. 마지막으로 개별 모델을 합해 최종 결과를 만든다. 선형 기본 학습기의 주요 차이점은 앙상블되는 각 모델이 선형이라는 것이다. 라소와 릿지가 규제(1장 참조)를 추가한 선형 회귀의 변형인 것처럼 gblinear도 선형 회귀에 규제 항을 추가한다. XGBoost의 창시자이자 개발자인 티엔치 첸은 gblinear를 여러번 부스팅하면 하나의 라소 회귀가 된다고 깃허브에서 언급했다.\n",
    "--- \n",
    "- gblinear는 로지스틱 회귀로 분류 문제에 사용할 수도 있다. 로지스틱 회귀도 선형 회귀처럼 최적의 계수(가중치)를 찾기 때문이다. 로지스틱 회귀는 시그모이드 함수를 통해 회귀의 출력을 확률로 변환한다.(1장 참조)\n",
    "---\n",
    "- 이장의 'gblinear 적용하기'절에서 gblinear에 대한 자세한 내용과 적용 예제를 다루어보겠다.\n",
    "- 이제 dart에 대해서 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620853be",
   "metadata": {},
   "source": [
    "# 8.1.2 DART\n",
    "- DART는 Dropouts meet Multiple Additive Regrresion Trees의 약자다. \n",
    "- 2015년 uc버클리의 K.V.라슈미와 마이크로소프트의 란 길라드-배크러치가 한 논문에서 소개했다.\n",
    "---\n",
    "- 라슈미와 길라드-배크러치는 MART( Multiple Additive Regrresion Trees)가 성공적인 모델이지만 이전 트리에 너무 많이 의존하는 문제가 있다고 강조했다. DART는 기본 규제 방법인 축소에 초점을 맞추는 신경망이 사용하는 드롭아웃 기법을 사용한다. 간단히 말하면 드롭아웃은 신경망의 각 층에 있는 유닛(수학적 연산 단위)을 훈련할 때 랜덤하게 삭제하여 과대학적합을 줄이는 방법이다. 다른 말로 하면 드롭아웃은 각 층에서 나오는 정보를 제거하여 학습 과정을 늦춘다.\n",
    "---\n",
    "- DART에서는 새로운 부스팅 단계마다 새로운 모델을 만들기 위해 모든 이전 트리의 잔차를 더하지 않고 이전 트리를 랜덤하게 선택하고 1/k 배율로 리프 노드를 정규화한다. 여기서 K는 드롭아웃된 트리의 개수이다.\n",
    "- DART는 결정트리의 변종이다. XGBoost의 DART구현은 드롭아웃을 위해 추가적인 매개변수가 있는 gbtree와 비슷하다.\n",
    "---\n",
    "- DART의 수학적인 상세 내용은 이 절의 서두에서 소개한 원본 논문을 참고해라."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f44075",
   "metadata": {},
   "source": [
    "# 8.1.3 XGBoost 랜덤 포레스트\n",
    "- 이 절에서 살펴볼 마지막 방법은 XGBoost 랜덤 포레스트이다.\n",
    "- XGBRegressor와 XGBClassifier의 num_parallel_tree를 1보다 큰 값을 설정하여 랜덤 포레스트를 기본 학습기로 사용할 수 있다. 또는 XGBoost의 XGBRegressor와 XGBRFClassifier 클래스를 사용해 랜덤 포레스트를 구현할 수 있다.\n",
    "--- \n",
    "- 그레이디언트 부스팅은 랜덤 포레스트 같이 강한 기본 학습기가 아니라 비교적 약한 기본 학습기의 오차를 향상시키기 위해 고안되었다는 점을 기억하자. 그럼에도 불구하고 랜덤 포레스트 기본 학습기가 도움이 될 수 있는 예외적인 경우가 있을 수 있다.\n",
    "---\n",
    "- 보너스로 XGBoost는 램덤 포레스트 머신러닝 알고리즘을 구현한 XGBRFRegressor와 XGBRFClassifier를 제공한다.\n",
    "- 이 클래스의 구현은 사이킷런의 랜덤 포레스트와 비슷하다. 주요한 차이점은 XGBoost는 과대적합을 방지하기 위한 기본 매개변수를 포함하고 있고 개별 트리를 만드는 방법이 다르다. XGBoost 랜덤 포레스트는 실험적인 단제이다. 하지만 2020년 후반부터 사이킷런의 랜덤 포레스트보다 뛰어난 성능을 내기 시작했다. 나중에 이 장에서 확인해보겠다.\n",
    "> 궁금한 점은 부스팅모델에서 배깅까지 다루면 성능이 좋은것인가?  \n",
    "---\n",
    "- 이 장의 마지막 절에서 XGBoost의 랜덤 포레스트를 기본 학습기와 스탠드얼론 방식으로 실험해보겠다.\n",
    "---\n",
    "- 이제 XGBoost 기본 학습게에 대해 알아보았으니 하나씩 이를 적용해보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52664b",
   "metadata": {},
   "source": [
    "# 8.2 gblinear 적용하기\n",
    "- 선형 모델에 잘 맞는 실제 데이터셋은 드물다. 실제 데이터셋은 깨끗하지 않고 트리 앙상블과 같은 복잡한 모델이 더 나은 결과를 낸다. 그렇지 않은 경우에는 선형 모델이 잘 일반화될 수 있다.\n",
    "- 머신러닝 알고리즘의 성공은 실제 데이터에 얼마나 잘 수행되는지에 달려 있다. 다음 절에서 먼저 당뇨병 데이터셋에 gblinear를 적용하고 그다음 합성된 선형 데이터에 적용해보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b2dff",
   "metadata": {},
   "source": [
    "## 8.2.1 gblinear를 당뇨병 데이터셋에 적용하기\n",
    "- 당뇨병 데이터셋은 당뇨병 환자 422명의 데이터로 구성된 회귀 데이터셋이며 사이킷런에 포함되어 있다.\n",
    "- 특성은 나이, 성별, BMI(체질량지수), BP(혈압), 여섯 개의 혈청 측정 값 등으로 구성된다. 타깃은 1년 후 당뇨병의 진행 상태다.\n",
    "---\n",
    "- 사이킷런에 포함된 데이터셋은 특성과 타깃으로 이미 나뉘어져 있다. 특성은 X, 타깃은 y로 로드하여 사용하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3aee233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경고 끄기\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import xgboost as xgb\n",
    "xgb.set_config(verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c78d7613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor, XGBClassifier, XGBRFRegressor, XGBRFClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f57c5c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load_diabetes() 함수에 return_X_y 매개변수를 true로 설정하여 특성 X와 y를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81c0d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load_diabetes는 함수를 호출하여 diabetes 데이터셋을 로드한다.\n",
    "# return_X_y = True 옵션을 사용하여 X와 y를 각각 독립 변수와 종속 변수로 분리한다.\n",
    "# 'X'는 데이터셋의 독립 변수들을 담고 있는 특성 행렬이며, 'y'는 해당 데이터셋의 종속 변수(타겟)을 담고 있는 벡터이다.\n",
    "\n",
    "X, y = load_diabetes(return_X_y = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7784e9",
   "metadata": {},
   "source": [
    "- cross_val_score()와 GridSearchCV를 사용할 계획이므로 동일하게 폴드를 나누기 위해 분할기 객체를 만들어 사용하겠다. 6장에서 훈련 세트와 테스트 세트의 타깃 클래스의 비율을 균등하게 유지하는 StratifiedKFold를 사용했다. 이 방법은 분류용이고 회귀에는 맞지 않다. 타깃이 연속적인 값이고 클래스가 없기 때문이다. 대신 **KFold 분할기**를 분할기를 사용해 클래스 비율을 고려하지 않고 일정하게 폴드 분할을 수행해보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02e252f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 이제 shuffle = True와 n_splits = 5 옵션으로 KFold 객체를 만든다.\n",
    "kfold = KFold(n_splits = 5, shuffle = True, random_state = 2) # stratifiedKFold를 사용안하고 KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9516e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 모델을 입력 받고 cv=kfold로 설정한 cross_val_score() 함수가 반환한 5 폴드 점수의 평균을 반환하는 함수를 만든다.\n",
    "def regression_model(model):\n",
    "    scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "    rmse = (-scores)**0.5\n",
    "    return rmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ef566c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.5355440650725"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 기본 학습기로 gblinear를 사용하기 위해 booster = 'gblinear'로 설정하여 XGBRegressor 객체를 만들고 regression_function()에 전달한다.\n",
    "regression_model(XGBRegressor(booster='gblinear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f907994a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.50936875436025"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. LinearRegression, L1규제를 사용하는 Lasso, L2규제를 사용하는 Ridge와 같은 다른 선형 모델로 점수를 계산해보겠다.\n",
    "# a) LinearRegression\n",
    "regression_model(LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8955a4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.64904114426349"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b) Lasso\n",
    "regression_model(Lasso())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0421b289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.835292374356676"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c) Ridge\n",
    "regression_model(Ridge())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f0eb8",
   "metadata": {},
   "source": [
    "결과에서 보듯이 gblinear를 기본 학습기로 사용한 XGBRegressor가 LinearRegression과 함께 가장 좋은 성능을 낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95b8eda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.9125519300286"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. booster = 'gbtree'로 설정한 XGBRegressor를 테스트해보자.\n",
    "regression_model(XGBRegressor(booster = 'gbtree'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ff04e",
   "metadata": {},
   "source": [
    "- **여기서 볼 수 있듯이 gbtree 기본 학습기는 gblinear 만큼 성능이 나오지 않는다.**\n",
    "- 이런 경우 선형 모델이 이상적이다.\n",
    "---\n",
    "- gblinear를 기본 학습기로 사용하고 매개변수를 바꿔서 성능을 높일 수 있는지 확인해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1a3b5",
   "metadata": {},
   "source": [
    "## gblinear 매개변수\n",
    "- 매개변수를 수정할 때 gblinear와 gbtree의 차이점을 이해하는 것이 중요하다.\n",
    "- 6장에서 소개한 많은 XGBoost 매개변수는 트리 매개변수이고 gblinear에 적용되지 않는다.\n",
    "- 예를 들어 Max_depth와 min_child_weight는 트리에 특화된 매개변수이다.\n",
    "---\n",
    "- 다음은 선형 모델을 위해 고안된 XGBoost gblinear의 매개변수를 요약한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b313b2f2",
   "metadata": {},
   "source": [
    "## reg_lambda\n",
    "- lambda는 파이썬 람다 함수의 예약어이기 때문에 사이킷런 API에서는 lambda 대신 reg_lambda를 사용한다.\n",
    "- 이는 Ridge에서 사용하는 L2 규제의 크기를 조정한다. 0에 가까운값이 잘 동작하는 경향이 있다.\n",
    "---\n",
    "- 기본값: 0\n",
    "- 범위: [0,inf]\n",
    "- 값을 증가시키면 과대적합을 방지한다.\n",
    "- 다른 이름: lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e53b2",
   "metadata": {},
   "source": [
    "## reg_alpha\n",
    "- 사이킷런 API는 reg_alpha와 alpha 매개변수를 모두 받을 수 있다. 이 매개변수는 Lasso에서 사용하는 L1 규제의 양을 조절한다. 0에 가까운 값이 잘 동작하는 경향이 있다.\n",
    "---\n",
    "- 기본값: 0\n",
    "- 범위: [0,inf]\n",
    "- 값을 증가시키면 과대적합을 방지한다.\n",
    "- 다른 이름: alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a48ba84",
   "metadata": {},
   "source": [
    "## updater\n",
    "- 부스팅 단계마다 선형 모델을 훈련하기 위해 XGBoost가 사용하는 알고리즘, shotgun은 Hogwild 병렬화 기반의 좌표 경사 하강법으로 비결정적인 솔루션을 만든다. 이와 다르게 coord_Descent는 일반적인 좌표 경사 하강법을 사용하여 결정적인 솔루션을 만든다.\n",
    "---\n",
    "- 기본값: shotgun\n",
    "- 범위: shotgun, coord_descent\n",
    "---\n",
    "> 좌표 경사 하강법은 한번에 하나의 좌표 그레이디언트를 찾아 오차를 최소화하는 알고리즘이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33763c9",
   "metadata": {},
   "source": [
    "## feature_selector\n",
    "- feature_selector는 좌표 경사 하강법의 가중치 업데이트 단계에서 특성의 순서를 선택하는 방법이다.\n",
    "---\n",
    "- a) cyclic - 가중치 업데이트 단계에서 특성을 순환하면서 선택한다.\n",
    "- b) shuffle - cyclic과 비슷하지만 가중치 업데이트 전에 특성을 랜덤하게 섞는다.\n",
    "- c) random - 특성을 랜덤하게 선택한다.\n",
    "- d) greedy - 그레이디언트가 가장 큰 특성을 선택한다. 속도가 느리다.\n",
    "- e) thrifty = greedy와 비슷하지만 가중치 업데이트 전에 그레이디언트 크기에 따라 특성을 정렬한다.\n",
    "- 기본값: cyclic\n",
    "다음과 같이 updater와 함께 사용해야 한다.\n",
    "- a) shotgun: cyclic, shuffle\n",
    "- b) coord_descent: random, greedy, thrifty\n",
    "\n",
    "> greedy는 대용량 데이터셋일 경우 계산 비용이 비싸다. 하지만(다음에 나오는) top_k 매개변수를 바꾸어 greedy 방식이 고려할 특성 개수를 줄일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efefb04f",
   "metadata": {},
   "source": [
    "## top_k\n",
    "- top_k는 greedy와 thrifty 방식에서 좌표 경사 하강법 동안에 선택하는 최상위 특성의 개수이다.\n",
    "---\n",
    "- 기본값: 0(모든 특성)\n",
    "- 범위: [0, 전체 특성 개수] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f28f1",
   "metadata": {},
   "source": [
    "## gblinear 그리드 서치\n",
    "- gblinear 매개변수 범위에 대해 알아보았으니 gridSearchCV를 사용한 grid_search() 함수로 최상의 모델을 찾아보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9159cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 6장과 비슷한 grid_search()함수를 만들어보겠다.\n",
    "def grid_search(params, reg = XGBRegressor(booster = 'gblinear')):\n",
    "    grid_reg = GridSearchCV(reg, params, scoring = 'neg_mean_squared_error', cv = kfold)\n",
    "    grid_reg.fit(X,y)\n",
    "    best_params = grid_reg.best_params_\n",
    "    print(\"최상의 매개변수:\", best_params)\n",
    "    best_score = np.sqrt(-grid_reg.best_score_)\n",
    "    print(\"최상의 점수:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "266b73b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최상의 매개변수: {'reg_alpha': 0.01}\n",
      "최상의 점수: 55.50014107605925\n"
     ]
    }
   ],
   "source": [
    "# 2. alpha 매개변수부터 바꾸어보자.\n",
    "grid_search(params={'reg_alpha':[0.001,0.01,0.1,0.5,1,5]})\n",
    "\n",
    "# 거의 점수가 동일하지만 아주 조금 좋아졌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10f84422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최상의 매개변수: {'reg_lambda': 0.001}\n",
      "최상의 점수: 56.1717001346381\n"
     ]
    }
   ],
   "source": [
    "# 3. 그 다음 같은 범위에서 reg_lambda를 바꿔보자\n",
    "grid_search(params = {'reg_lambda':[0.001,0.01,0.1,0.5,1,5]})\n",
    "\n",
    "# 점수가 비슷하지만 조금 나빠졌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d221b8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최상의 매개변수: {'feature_selector': 'shuffle'}\n",
      "최상의 점수: 55.47401131663279\n"
     ]
    }
   ],
   "source": [
    "# 4. 그다음 update와 함께 featur_selector를 탐색해보겠다.\n",
    "# 기본값은 updater = shotgun와 feature_selector=cyclic이다.\n",
    "# updater = shotgun일 때 feature_selector에 가능한 다른 옵션은 shuffle뿐이다.\n",
    "# shuffle이 cyclic보다 더 나은 성을 내는지 확인해보자.\n",
    "grid_search(params ={'feature_selector':['shuffle']})\n",
    "\n",
    "# 여기에서는 shuffle이 더 성능이 높지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f7373d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최상의 매개변수: {'feature_selector': 'thrifty', 'updater': 'coord_descent'}\n",
      "최상의 점수: 55.488143951136536\n"
     ]
    }
   ],
   "source": [
    "# 5. 이제 updater를 coord_descent로 바꾸어보겠다.(좌표 경사 하강법)\n",
    "# feature_selector는 random, greedy, thrifty가 가능하다.\n",
    "# 다음 코드처럼 grid_search()함수로 모든 feature_selector를 테스트해보겠다.\n",
    "grid_search(params = {'feature_selector': ['random','greedy','thrifty'],\n",
    "                     'updater':['coord_descent']})\n",
    "# 점수가 조금 나아졌다.\n",
    "# 마지막으로 확인할 매개변수는 top_k이다. \n",
    "# 이 매개변수는 좌표 경사 하강법에서 greedy와 thifty방식이 사용할 특성의 개수를 결정한다.\n",
    "# 전체 특성 개수는 10이므로 2~9 사이를 탐색해보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e682aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최상의 매개변수: {'feature_selector': 'thrifty', 'top_k': 3, 'updater': 'coord_descent'}\n",
      "최상의 점수: 55.47871836076556\n"
     ]
    }
   ],
   "source": [
    "# 6. feature_selector를 greedy와 thrifty로 설정하고 top_k 범위를 지정하여 grid_search() 함수를 호출한다.\n",
    "grid_search(params = {'feature_selector':['greedy', 'thrifty'],\n",
    "                     'updater':['coord_descent'], 'top_k':[3,5,7,9]})\n",
    "\n",
    "# 이 점수가 지금까지 최상이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ed04d1",
   "metadata": {},
   "source": [
    "### 비대칭 그리드 탐색\n",
    "- 본문에서는 updater가 shotgun일 때와 coord_descent일 때를 따로 나누어 그리드 서치를 수행했지만 탐색 범위를 지정한 두 딕셔너리를 리스트로 연결하여 동시에 탐색할 수 있다. 이런 방식을  비대칭 그리드 탐색이라고 부른다. 먼저 매개변수 그리드를 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54bac58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'updater':['shotgun'],\n",
    "              'featrue_selector':['cyclic','shuffle']},\n",
    "             {'updater':['coord_descent'],\n",
    "             'feature_selector':['random','greedy','thrifty']}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be734943",
   "metadata": {},
   "source": [
    "- updater의 기본값이 shotgun이지만 여기에서는 이해하기 쉽도록 명시적으로 지정했다.\n",
    "- 이렇게 지정하면 GridSearchCV 클래스는 shotgun일 때 cyclic과 shuffle을 테스트하고 coord_descent일 때 random, greedy, thrifty를 테스트한다. 이 매개변수 그리드로 grid_search()함수를 호출해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c9f0680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최상의 매개변수: {'feature_selector': 'thrifty', 'updater': 'coord_descent'}\n",
      "최상의 점수: 55.488143951136536\n"
     ]
    }
   ],
   "source": [
    "grid_search(params = param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07a1a53",
   "metadata": {},
   "source": [
    "## 8.2.2 선형 데이터셋\n",
    "- 선형 데이터셋을 직접 만들어보겠다. X의 범위를 1~99로 선택하고 약간의 무작위성을 추가하여 선형적으로 y값을 만들겠다.\n",
    "---\n",
    "- 선형 데이터셋을 만드는 과정은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c278bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. X범위를 1~99사이로 지정한다.\n",
    "X = np.arange(1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63af11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 동일한 결과를 만들기 위해 넘파이 랜덤 시드를 설정한다.\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7006f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. y를 빈 리스트로 만든다.\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04b9ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. X를 순회하면서 -0.2~0.2 사이의 난수를 곱하여 y에 추가한다.\n",
    "for i in X:\n",
    "    y.append(i*np.random.uniform(-0.2,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "141aba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. y를 넘파이 배열로 변환한다.\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "daf48c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 사이킷런 API는 샘플이 행을 따라 늘어선 2차원 배열을 기대하기 때문에 X와 y를 하나의 열을 가진 2차원 배열로 변환한다.\n",
    "X = X.reshape(X.shape[0],1)\n",
    "y = y.reshape(y.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90d2865c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.025602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.379259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.059595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.103484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.159264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-10.843842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-3.196668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5.581067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>6.330068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-13.049106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      column1\n",
       "0   -0.025602\n",
       "1   -0.379259\n",
       "2    0.059595\n",
       "3   -0.103484\n",
       "4   -0.159264\n",
       "..        ...\n",
       "94 -10.843842\n",
       "95  -3.196668\n",
       "96   5.581067\n",
       "97   6.330068\n",
       "98 -13.049106\n",
       "\n",
       "[99 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y, columns=['column1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ca250a",
   "metadata": {},
   "source": [
    "- 사이킷런은 2차원 배열의 특성과 1차원 배열의 타깃을 기대한다. \n",
    "- 따라서 y를 2차원 배열로 바꿀 필요가 없으며 파이썬 리스트를  사용해도 괜찮다.\n",
    "- 넘파이 reshape() 메서드에서 -1로 지정하면 결정되지 않은 모든 차원을 사용한다. 따라서 X.reshape(-1,1)과 같이 쓸 수 있다.\n",
    "---\n",
    "- booster가 gblinear일 경우 훈련된 선형 모델의 계수(가중치)와 절편이 coef_, intercept_속성에 저장된다.\n",
    "- 예제 데이터셋이 한 개의 특성을 사용하므로 X-y 2차원 평면에 산점도를 그린 후에 모델이 찾은 직선을 그릴 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6a3a664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xgbr = XGBRegressor(booster='gblinear')\n",
    "xgbr.fit(X, y)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot((0, 99), (xgbr.intercept_, xgbr.coef_*99 + xgbr.intercept_))\n",
    "plt.show()\n",
    "\n",
    "# plt.plot((0, 99), (xgbr.intercept_, xgbr.coef_*99 + xgbr.intercept_)): \n",
    "# 이 줄은 XGBRegressor 모델의 절편(intercept)과 계수(coefficient)를 사용하여 선형 회귀선을 그리는 부분입니다.\n",
    "# plt.plot() 함수를 사용하여 선을 그리는데, 첫 번째 인자 (0, 99)는 선의 끝점의 X 좌표를 지정하고, \n",
    "# 두 번째 인자 (xgbr.intercept_, xgbr.coef_*99 + xgbr.intercept_)는 선의 끝점의 Y 좌표를 지정합니다. \n",
    "# ㅡ선의 방정식은 y = xgbr.coef_ * x + xgbr.intercept_로 계산됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f434ae7a",
   "metadata": {},
   "source": [
    "Y 좌표가 계산되는 방식을 이해하기 위해 두 번째 인수를 더 분석해 보겠습니다.\n",
    "\n",
    "xgbr.intercept_: XGBRegressor 모델에서 얻은 선형회귀선의 절편항입니다. \n",
    "X(독립 변수)가 0일 때 Y(종속 변수)의 값을 나타냅니다. 절편은 선이 Y축과 교차하는 지점입니다.\n",
    "xgbr.coef_: XGBRegressor 모델에서 얻은 선형회귀선의 계수항입니다. X의 단위 변화에 대해 Y가 얼마나 많이 변하는지 나타내는 선의 기울기를 나타냅니다. 선의 기울기를 결정합니다.\n",
    "xgbr.coef_*99 + xgbr.intercept_: X 값 99에 해당하는 끝점의 Y 좌표를 계산하는 부분입니다. 계수(xgbr.coef_)에 99(X 값)를 곱하여 계산합니다. Y 좌표에 대한 기울기의 기여도, 그리고 최종 Y 좌표 값을 얻기 위해 절편(xgbr.intercept_)을 추가합니다.\n",
    "(0, 99)를 X 좌표로 지정하고 (xgbr.intercept_, xgbr.coef_*99 + xgbr.intercept_)를 Y 좌표로 지정하여 plt.plot() 함수는 이 두 점을 연결합니다. 직선을 그립니다. 방정식 y = xgbr.coef_ * x + xgbr.intercept_는 이 선의 수학 공식을 나타냅니다. XGBRegressor 모델에서 학습한 계수를 기반으로 Y가 X와 선형적으로 어떻게 변하는지 설명합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5896cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.214946302686012"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model(XGBRegressor(booster = 'gblinear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9346b4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.372359516507444"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model(XGBRegressor(booster = 'gbtree'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cfc7b3",
   "metadata": {},
   "source": [
    "- 여기서 볼 수 있듯이 선형 데이터셋에서 gblinear가 훨씬 좋은 성능을 낸다.\n",
    "- 동일한 데이터셋에 사이킷런의 LinearRegression 클래스를 적용하여 비교해보죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36c2f1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.214962315808842"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model(LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e30354b",
   "metadata": {},
   "source": [
    "## 8.2.3 gblinear 분석\n",
    "- gblinear는 강력한 옵션이지만 선형 모델이 트리 기반 모델보다 더 높은 성능을 낼 수 있다는 확신이 있을 때만 사용해야 한다.\n",
    "- gblinear는 Linear는 LinearRegression과 거의 동일한 수준의 성능을 낸다. XGBoost에서 데이터셋이 크고 선형적일 때 gblinear가 기본 학습기로 좋은 선택이다. gblinear는 분류에서도 사용할 수 있다. 다음 절에서 적용해보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230f8b9",
   "metadata": {},
   "source": [
    "## 8.3 비교하기\n",
    "- dart 기본 학습기는 둘 다 그레이디언트 부스팅 트리라는 점에서 gbtree와 비슷하다. dart는 부스팅 단계마다 트리를 삭제(드롭아웃이라고 부른다)하는 것이 주요한 차이점이다. 이 절에서 회귀와 분류 문제를 사용해 dart와 다른 기본 학습기를 비교해보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffde00ad",
   "metadata": {},
   "source": [
    "## 8.3.1 dart를 사용한 XGBRegressor\n",
    "- 당뇨병 데이터셋에서 dart의 성능을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9ac89fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 먼저 이전처럼 load_diabetes() 함수를 사용해 X와 y를 로드한다.\n",
    "X,y = load_diabetes(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5661c862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.60272785630248"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. booster = 'dart'로 설정해 dart를 기본 학습기로 사용하는 XGBRegressor를 regression_model()함수에 전달한다.\n",
    "regression_model(XGBRegressor(booster = 'dart', rate_drop = 0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42fc882",
   "metadata": {},
   "source": [
    "- booster = 'dart'일 때 rate_drop 매개변수는 드롭아웃될 확률을 지정한다.\n",
    "- rate_drop = 0.5로 지정하여 gbtree보다 좋은 성능을 냈다.\n",
    "- 분류 데이터셋에서도 dart와 gbtree를 비교해보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d6a3de",
   "metadata": {},
   "source": [
    "## 8.3.2 dart를 사용한 XGBClassifier\n",
    "- 이 책에서 인구 조사 데이터셋을 여러 장에서 사용했다. 1장에서 이 데이터셋을 정제한 버전이 8장의 깃허브 폴더에 포함되어 있다.\n",
    "- 이 데이터셋을 사용해 dart의 성능을 테스트해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bd067ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 인구 조사 데이터셋을 데이터프레임으로 로드하고 마지막 열을 제외한 모든 열을 특성 X로 만들고 마지막 열을 타깃 y로 만든다.\n",
    "df_census = pd.read_csv('census_cleaned.csv')\n",
    "X_census = df_census.iloc[:, :-1]\n",
    "y_census = df_census.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92692f64",
   "metadata": {},
   "source": [
    "이 장의 서두에서 정의한 regression_model()함수와 비슷하게 모델을 입력으로 받아 cross_val_score()의 결과 점수를 평균하여 출력하는 함수를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f1497e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state= 2)\n",
    "\n",
    "def classification_model(model):\n",
    "    scores = cross_val_score(model, X_census, y_census, scoring = 'accuracy', cv=skf)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06be1e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8711649041738863"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 이제 booster = 'gbtree'와 booster = 'dart'로 설정한 XGBClassifier를 사용해 함수를 호출하고 결과를 비교한다.\n",
    "# 데이터셋이 크기 때문에 실행 시간이 이전보다 오래 걸릴 것이다.\n",
    "# a) booster = 'gbtree'일 때\n",
    "classification_model(XGBClassifier(booster = 'gbtree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e42df2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8702743586725623"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b) booster = 'dart'일 때\n",
    "classification_model(XGBClassifier(booster = 'dart', rate_drop = 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b35ee",
   "metadata": {},
   "source": [
    "- 드롭아웃 확률을 0.1로 지정하여 아주 조금 정확도를 높였다.\n",
    "- 그다음 dart와 gblinear를 비교해보겠다. \n",
    "- gblinear는 분류에도 사용할 수 있다.\n",
    "- 로지스틱 회귀처럼 가중치 합에 시그모이드 함수를 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "62aa5e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8512023816664535"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. booster = 'gblinear'로 지정한 XGBClassifier로 Classification_model()함수를 호출한다.\n",
    "classification_model(XGBClassifier(booster = 'gblinear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c18a2e",
   "metadata": {},
   "source": [
    "- 선형 기본 학습기의 성능이 트리 기반 학습기보다 낮다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "599f8aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7959523072547026"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. gblinear와 로지스틱 회귀를 비교해보자. \n",
    "# 데이터셋이 크기 때문에 LogisticRegression 클래스의 max_iter 매개변수를 기본값 100에서 1000으로 바꿔 수렴하기 위한 반복횟수를 늘려준다.\n",
    "# 여기에서는 max_iter를 증가시키면 정확도가 향상된다.\n",
    "classification_model(LogisticRegression(max_iter = 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efb3c2e",
   "metadata": {},
   "source": [
    "- gblinear가 로지스틱 회귀보다 확실히 더 좋다. \n",
    "- 분류에서 XGBoost의 gblinear가 로지스틱 회귀의 가능한 대안이라는 점을 꼭 기억해라."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d741b907",
   "metadata": {},
   "source": [
    "- 기본 학습기로써 dart를 gbtree나 gblinear와 비교해보았다. 이제 dart 매개변수를 조정해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed6034",
   "metadata": {},
   "source": [
    "## 8.3.3 dart 매개변수\n",
    "- dart는 gbtree 매개변수를 모두 포함하고 드롭아웃 비율과 같은 매개변수를 추가로 제공한다.\n",
    "- 전체 매개변수에 대한 자세한 정보는 공식 문서를 참고하자.\n",
    "- 다음은 dart에 추가되는 XGBoost 매개변수를 요약한 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc72df",
   "metadata": {},
   "source": [
    "### sample_type\n",
    "- sample_type의 옵션은 균등하게 드롭아웃하는 uniform과 가중치에 비례하여 드롭하웃하는 weighted가 있다.\n",
    "---\n",
    "- 기본값: 'uniform'\n",
    "- 범위: ['uniform', 'weighted']\n",
    "- 드롭아웃될 트리를 선택하는 방법을 결정한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1416212",
   "metadata": {},
   "source": [
    "### normalize_type\n",
    "- normalize_type의 옵션은 새로운 트리가 드롭아웃된 트리와 같은 가중치를 가지는 tree와 새로운 트리가 드롭아웃된 트리의 합과 동일한 가중치를 가지는 forest가 있다.\n",
    "---\n",
    "\n",
    "- 기본값: 'tree'\n",
    "- 범위: ['tree', 'forest']\n",
    "- 트리의 가중치를 정규화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0d84c7",
   "metadata": {},
   "source": [
    "### rate_drop\n",
    "- rate_drop은 드롭아웃될 트리의 비율을 지정한다.\n",
    "---\n",
    "- 기본값: 0.0\n",
    "- 범위: [0.0, 1.0]\n",
    "- 드롭아웃될 트리 비율이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db24b1c4",
   "metadata": {},
   "source": [
    "### one_drop\n",
    "- One_drop을 1로 지정하면 부스팅 단계에서 적어도 하나의 트리가 항상 드롭아웃된다.\n",
    "---\n",
    "- 기본값: 0\n",
    "- 범위: [0,1]\n",
    "- 드롭아웃될 트리가 하나도 선택되지 않을  경우 최소한 하나의 트리를 드롭아웃한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e4c28",
   "metadata": {},
   "source": [
    "### skip_drop\n",
    "- skip_drop은 드롭아웃을 건너 뛸 확률을 지정한다. skip_drop은 rate_rop이나 one_drop보다 높은 우선순위를 가진다.\n",
    "- 기본적으로 각 트리는 드롭아웃될 확률이 동일하다. 따라서 한 부스팅 단계에서 어떤 트리도 드롭아웃되지 않을 가능성이 있다.\n",
    "- skip_drop은 이 확률을 조정하여 드롭아웃 횟수를 조절한다.\n",
    "---\n",
    "- 기본값: 0.0\n",
    "- 범위: [0.0, 1.0]\n",
    "- 드롭아웃을 건너 뛸 확률을 지정한다.\n",
    "\n",
    "- 이제 dart 매개변수를 바꾸고 점수를 비교해보겠다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f2dba5",
   "metadata": {},
   "source": [
    "### 8.3.4 dart 매개변수 적용\n",
    "- 부스팅 단계마다 최소한 하나의 트리를 드롭아웃하기 위해서 one_drop = 1로 지정한다.\n",
    "- classification_model() 함수를 사용해 인구 조사 데이터셋에 이를 적용해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2c67987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8719940529072264"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model(XGBClassifier(booster = 'dart', one_drop = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cfa507",
   "metadata": {},
   "source": [
    "- 정확도가 0.1 퍼센트 포인트 높아졌다. 이는 부스팅 단계마다 적어도 하나의 트리를 드롭아웃하는 것이 도움이 된다는 뜻이다.\n",
    "- 드롭아웃을 적용해 모델의 성능이 달라졌다. 이제 작고 빠른 당뇨병 데이터셋을 사용해 다른 매개변수를 적용해보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56acc83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.78061696170955"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. regression_model()함수를 사용해 sample_typedmf uniform에서 weighted로 바꾼다.\n",
    "regression_model(XGBRegressor(booster = 'dart', rate_drop = 0.5,\n",
    "                             sample_type = 'weighted'))\n",
    "\n",
    "# 앞서 기본값 unifrom보다 오차가 조금 더 커졌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e5fb114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.35920574990998"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. normalize_type을 forest로 바꾸어보겠다.\n",
    "regression_model(XGBRegressor(booster = 'dart', rate_drop = 0.5,\n",
    "                             normalize_type = 'forest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0e3c7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.79383589165782"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. one_drop =1로 지정하여 부스팅 단계마다 최소한 하나의 트리를 드롭아웃시켜보겠다.\n",
    "regression_model(XGBRegressor(booster = 'dart', one_drop = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f89001",
   "metadata": {},
   "source": [
    "- 드롭아웃할 트리의 확률을 지정하는 rate_drop은 다음처럼 grid_search()함수로 최적의 값을 찾을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c804c92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최상의 매개변수: {'rate_drop': 0.2}\n",
      "최상의 점수: 61.046435908538385\n"
     ]
    }
   ],
   "source": [
    "grid_search(params = {'rate_drop':[0.01, 0.1, 0.2, 0.4]},\n",
    "           reg = XGBRegressor(booster = 'dart', one_drop = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e45d1",
   "metadata": {},
   "source": [
    "- 이 점수는 지금까지 가장 좋은 점수이다.\n",
    "- 부스팅 단계에서 드롭아웃하지 않을 확률을 지정하는 skip_drop으로도 비슷한 탐색을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "298f8355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최상의 매개변수: {'skip_drop': 0.1}\n",
      "최상의 점수: 62.929666979954796\n"
     ]
    }
   ],
   "source": [
    "grid_search(params = {'skip_drop': [0.01, 0.1, 0.2, 0.4]},\n",
    "           reg = XGBRegressor(booster = 'dart', one_drop =1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c97a1",
   "metadata": {},
   "source": [
    "## 8.3.5 dart 분석\n",
    "- dart는 XGBoost 프레임워크의 강력한 옵션이다. dart는 gbtree의 매개변수를 모두 사용할 수 있기 때문에 매개변수를 튜닝할 때 기본 학습기를 gbtree에서 dart로 쉽게 바꿀 수 있다. one_drop, rate_drop, normalize_type등과 같은 새로운 매개변수를 실험하여 성능을 높일 수 있는 장점이 있다. dart는 XGBoost를 사용한 연구와 모델 구축에서 실험해볼 가치가 있는 기본학습기다.\n",
    "---\n",
    "dart에 대해 잘 이해했으므로 랜덤 포레스트로 넘어가 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa14ab14",
   "metadata": {},
   "source": [
    "## 8.4 XGBoost 랜덤 포레스트\n",
    "- XGBoost에서 랜덤 포레스트를  구현하는 방법은 두 가지이다. \n",
    "- 첫째는 랜덤 포레스트를 기본 학습기로 사용하는 것이다. 둘째는 스탠드얼론 방식인 XGBRFRegressor와 XGBRFClassifier를 사용하는 것이다. 먼저 기본학습기로 랜덤포레스트를 사용해보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10898da",
   "metadata": {},
   "source": [
    "# 8.4.1 랜덤 포레스트 기본 학습기\n",
    "- 랜덤 포레스트 기본 학습기는 booster 매개변수에서 지정하지 않는다.\n",
    "- num_parallel_tree 매개변수를 기본값 1보다 크게 지정하면 gbtree(또는 dart)를 부스팅 랜덤 포레스트로 바꾼다.\n",
    "- 즉 부스팅 단계마다 하나의 트리가 아니라 여러 개의 트리를 사용하여 앙상블을 구성한다.\n",
    "- 다음은 num_parallel_tree 매개변수를 간략하게 정리한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc8c1db",
   "metadata": {},
   "source": [
    "### num_parallel_tree\n",
    "- num_parallel_tree는 부스팅 단계에서 만들 트리 개수를 지정한다.\n",
    "---\n",
    "- 기본값:1\n",
    "- 범위: [1, inf]\n",
    "- 한 부스팅 단계에서 만들 트리 개수\n",
    "- 1보다 크게 지정하면 부스팅 랜덤 포레스트가 된다.\n",
    "---\n",
    "- XGBoost의 랜덤 포레스트 기본 학습기가 실제 어떻게 동작하는지 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2fd086d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.95866495368594"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. booster = 'gbtree'로 지정한 XGBRegressor로 regression_model() 함수를 호출한다. 추가적으로 \n",
    "# num_parallel_tree = 25으로 설정하여 부스팅 단계마다 25개의 트리를 구성한다.\n",
    "\n",
    "regression_model(XGBRegressor(booster = 'gbtree', num_parallel_tree = 25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab06c4c",
   "metadata": {},
   "source": [
    "- 이 경우에 부스팅 단계마다 하나의 트리를 사용하는 gbtree와 거의 동일한 성능을 낸다. 그레이디언트 부스팅은 이전 트리의 실수로부터 학습하도록 고안되었다. 강력한 랜덤 포레스트로 부스팅을 시작하면 학습할 것이 없거나 아주 적다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac266a2d",
   "metadata": {},
   "source": [
    "- 랜덤 포레스트를 XGBoost의 기본 학습기로 사용하는 방법을 알아보았으므로 이제 스탠드얼론 방식의 랜덤 포레스트를 만들어보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60445380",
   "metadata": {},
   "source": [
    "##  8.4.2 스탠드얼론 랜덤 포레스트\n",
    "- XGBoost는 XGBRegressor와 XGBClassifier 외에도 랜덤 포레스트를 위한 XGBRFRegressor와 XGBRFClassifier 클래스를 제공한다.\n",
    "---\n",
    "- XGBoost 공식 문서에 따르면 랜덤 포레스트 사이킷런 API는 아직 실험적이며 향후 인터페이스가 바뀔 수 있다. 이글을 쓰는 시점에 XGBRFRegressor와 XGBRFClassifier는 다음 매개변수를 포함하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb14d7",
   "metadata": {},
   "source": [
    "#### n_estimators\n",
    "- XGBRFRegressor나 XGBRClassifier를 사용하여 랜덤 포레스트를 만들 때는 num_parallel_tree가 아니라 n_estimators를 사용한다. XGBRFRegressor와 XGBRFClassifier는 그레이디언트 부스팅이 아니라 전통적인 랜덤 포레스트처럼 배깅 방식이다.\n",
    "---\n",
    "- 기본값: 100\n",
    "- 범위: [1, inf]\n",
    "- 랜덤 포레스트를 위해 num_parallel_tree로 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c74c3",
   "metadata": {},
   "source": [
    "#### learning_rate\n",
    "- learning_rate는 일반저긍로 한 단계의 부스팅을 사용하는 XGBRFRegressor나 XGBRFClassifier가 아니라 부스터를 포함하는 모델을 위한 것이다. 그럼에도 불구하고 learning_rate를 기본값 1에서 바꾸면 결과가 달라지기 때문에 일반적으로 이 매개변수를 조정하는 것은 권장되지 않는다.\n",
    "---\n",
    "- 기본값: 1\n",
    "- 범위: [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad6a6e",
   "metadata": {},
   "source": [
    "#### subsample, colsample_by_node\n",
    "- 두 매개변수의 기본값은 0.8이므로 XGBRFRegressor와 XGBRFClassifier가 기본적으로 과대작합될 가능성이 적다.\n",
    "- 이것이 XGBoost와 사이킷런의 랜덤 포레스트 구현의 큰 차이중 하나다.\n",
    "- 기본값\n",
    "- 범위:[0,1]\n",
    "- 값을 줄이면 과대적합을 만든 데 도움이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "668e7536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.43613846827259"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 먼저 regression_model()함수에 XGBRFRegressor 객체를 전달한다.\n",
    "regression_model(XGBRFRegressor())\n",
    "\n",
    "# 이 점수는 gbtree모델보다 조금 더 좋고 이 장에서 구한 가장 좋은 선형 모델보다는 조금 나쁘다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "833933ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.44526192030271"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 비교를 위해 같은 함수에 RandomForestRegressor를 전달하여 성능을 확인해보겠다.\n",
    "regression_model(RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a2a03",
   "metadata": {},
   "source": [
    "- 이번에는 대용량 분류 데이터인 인구 조사 데이터셋에서 XGBoost의 랜덤 포레스트와 사이킷런의 랜덤 포레스트를 비교해보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bfbee929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8554712913994351"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. classification_model 함수에 XGBRFClassifier 객체를 전달한다.\n",
    "classification_model(XGBRFClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55798cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8570682062448529"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 이번에는 randomForestClassifier를 사용해서 결과를 비교해보겠다.\n",
    "classification_model(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da1a4b",
   "metadata": {},
   "source": [
    "### XGBoost의 랜덤 포레스트 구현 비교\n",
    "- XGBoost의 랜덤 포레스트 구현은 그레이디언트 부스팅 구현을 그대로 사용하면서 한 번의 부스팅 단계에서 여러 개의 트리를 만드는 식이다. XGBoost의 기본 파이썬 API의 경우 num_boost_round = 1, num_parallel_tree를 1보다 크게 설정한다. 랜덤포레스트처럼 특성을 랜덤 샘플링하기 위해 colsample_bynode를 1보다 작게 설정한다. 학습률은 eta는 1로 설정해야한다.\n",
    "---\n",
    "- XGBRFRegressor와 XGBRFCLassifier 클래스는 XGBRegressor와 XGBClassifier를 상속하고 n_estimators 값을 파이썬 API의 num_parallel_tree로 설정하고 num_boost_round를 1로 고정하는 식으로 구현되어 있다.(XGBRegressor와 XGBClassifier는 n_estimators 값을 num_boost_round로 사용한다.) 또한 기본적으로 subsample = 0.8, learning_rate = 1.0, colsample_bynode = 0.8을 사용한다. 이런 랜덤 포레스트 구현이 스탠드얼론 방식이다.\n",
    "---\n",
    "- 랜덤 포레스트를 기본 학습기로 사용하기 위해서는 num_parallel_tree를 1보다 크게 설정한다.\n",
    "- 이 때 만들어지는 총 트리 개수는 num_boost_round * num_parallel_tree(사이킷런 API의 경우 n_estimators * num_parallel_tree)이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87305948",
   "metadata": {},
   "source": [
    "## 8.4.3 XGBoost 랜덤 포레스트 분석\n",
    "- num_parallel_tree를 1보다 크게 설정하여 언제든지 랜덤 포레스트를 XGBoost의 기본 학습기로 테스트해볼 수 있다. 하지만 부스팅은 강한 모델이 아니라 약한 모델로부터 학습되도록 고안되었기 때문에 num_parallel_tree를 1에 가까운 값으로 설정해야한다. 랜덤 포레스트를 기본 학습기로 사용하는 일은 많지 않다. 하나의 트리를 부스팅해서 최적의 점수를 얻지 못하면 랜덤 포레스트 기본 학습기가 대안일 수 있다.\n",
    "---\n",
    "- 또한 XGBoost의 랜덤 포레스트 구현인 XGBRFRegressor와 XGBRFClassifier는 사이킷런의 랜덤 포레스트 대신 사용할 수 있다. XGBoost의 XGBRFRegressor와 XGBRFClassifier는 사이킷런의 RandomForestRegressor, RandomForestClassifier와 거의 비슷한 성능을 제공한다. 머신러닝 커뮤니티에서 XGBoost의 전반적인 성공을 감안할 때 XGBRFRegressor와 XGBRFClassifier를 사용해볼 가치가 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e228bfa0",
   "metadata": {},
   "source": [
    "### 마치며\n",
    "- 이 장에서 gbtree, dart, gblinear, 랜덤 포레스트 같은 XGBoost의 모든 기본 학습기를 회귀와 분류 데이터셋에 적용해보면서 XGBoost에 대한 지식을 크게 넓혔다. 성능을 높이기 위해 기본 학습기마다 고유한 매개변수를 살펴보고, 튜닝해보았다. 선형 데이터셋에 gblinear를 적용해보고, XGBRFRegressor와 XGBRFClassifier를 사용해 부스팅이 없는 XGBoost 랜덤 포레스트를 만들었다. 모든 기본 학습기를 다루어보았으므로 XGBoost를 다루는 수준이 한 층 높아졌다.\n",
    "--- \n",
    "- 다음 장에서는 캐글 마스터의 팁과 기법을 분석하여 XGBoost 기술을 더욱 발전시켜보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d920c03e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea4484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4cf468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9865fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e730c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef41b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c2a48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61360416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5de1c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5da55d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9aa8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b0ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a801742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b008887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7356347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c9880c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b7c90f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
